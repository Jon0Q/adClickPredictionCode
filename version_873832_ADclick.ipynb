{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "version_02_ADclick.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PU2jA5ayfVuo",
        "outputId": "0947af4e-90a1-47d7-dd3a-970ac440a6e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2AqxF-Oqft6m",
        "outputId": "5a997d93-e2ba-499b-8ed3-eda4b4bb923f"
      },
      "source": [
        "%cd /content/drive/MyDrive/aaaaa/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/aaaaa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ub8O2cfQiCAJ",
        "outputId": "0462d2cf-d39e-44a3-dfd2-efbdc85a100b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json  sampleSubmission.gz  test.gz     train.csv  train_v2.csv\n",
            "rtrain.csv   test.csv\t\t  train2.csv  train.gz\t tree.dot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMiNuofo961d"
      },
      "source": [
        "features = ['id', 'state', 'C22', 'C1', 'banner_pos', 'device_id', 'device_ip',\n",
        "      'site_category', 'app_id', 'app_domain', 'app_category', 'C23',\n",
        "       'C24', 'device_model', 'device_type', 'device_conn_type', 'C14',\n",
        "      'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bEzaqLxFU3V"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NHIS2h722-Oz",
        "outputId": "d44284a7-8c4f-4c43-c473-8adf4ce59182"
      },
      "source": [
        "## in import les les Biblios\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "n_rows = 300000\n",
        "df = pd.read_csv(\"rtrain.csv\", nrows=n_rows)\n",
        "\n",
        "\n",
        "X = df.drop(['state', 'id', 'C22', 'C23', 'C24'], axis=1).values\n",
        "Y = df['state'].values\n",
        "\n",
        "## pour commencer on va \n",
        "\n",
        "n_train = 100000\n",
        "X_train = X[:n_train]\n",
        "Y_train = Y[:n_train]\n",
        "X_test = X[n_train:]\n",
        "Y_test = Y[n_train:]\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "X_train_enc = enc.fit_transform(X_train)\n",
        "\n",
        "X_test_enc = enc.transform(X_test)\n",
        "\n",
        "# # avec la scikit-learn \n",
        "from sklearn.linear_model import SGDClassifier\n",
        "sgd_lr = SGDClassifier(loss='log', penalty=None, fit_intercept=True, max_iter=100, learning_rate='constant', eta0=0.01)\n",
        "sgd_lr.fit(X_train_enc.toarray(), Y_train)\n",
        "\n",
        "pred = sgd_lr.predict_proba(X_test_enc.toarray())[:, 1]\n",
        "print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------------------------------\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------\n",
        "\n",
        "## la selection de features en utilisant la L1 regularization\n",
        "sgd_lr_l1 = SGDClassifier(loss='log', penalty='l1', alpha=0.0001, fit_intercept=True, max_iter=100, learning_rate='constant', eta0=0.01)\n",
        "sgd_lr_l1.fit(X_train_enc.toarray(), Y_train)\n",
        "\n",
        "coef_abs = np.abs(sgd_lr_l1.coef_)\n",
        "print(coef_abs)\n",
        "\n",
        "\n",
        "## les 10 features les moins utilisables \n",
        "print(np.sort(coef_abs)[0][:10])\n",
        "\n",
        "feature_names = enc.get_feature_names()\n",
        "bottom_10 = np.argsort(coef_abs)[0][:10]\n",
        "print('les10 features les moins importantes sont :\\n', feature_names[bottom_10])\n",
        "\n",
        "\n",
        "## les 10 meilleurs features \n",
        "print(np.sort(coef_abs)[0][-10:])\n",
        "top_10 = np.argsort(coef_abs)[0][-10:]\n",
        "print('les10 features les plus importantes sont ::\\n', feature_names[top_10])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples: 100000, AUC on testing set: 0.732\n",
            "[[0.         0.13803421 0.         ... 0.         0.         0.13455693]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "les10 features les moins importantes sont :\n",
            " ['x0_1001' 'x8_84dda655' 'x8_84c2f017' 'x8_84ace234' 'x8_84a9d4ba'\n",
            " 'x8_84915a27' 'x8_8441e1f3' 'x8_840161a0' 'x8_83fbdb80' 'x8_83fb63cd']\n",
            "[0.6546022  0.74700015 0.76492194 0.81732684 0.92023462 0.99551065\n",
            " 0.99600743 1.06115632 1.09704979 1.33308764]\n",
            "les10 features les plus importantes sont ::\n",
            " ['x7_cef3e649' 'x3_7687a86e' 'x18_61' 'x18_15' 'x5_5e3f096f' 'x5_9c13b419'\n",
            " 'x2_d9750ee7' 'x2_763a42b5' 'x3_27e3c518' 'x5_1779deee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB6M8RnBDUJb"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fCQhCLJsDYqd",
        "outputId": "f2a70137-df2b-4d51-d15c-18fda5c1d735"
      },
      "source": [
        "\n",
        "# dans cette partie on va aborder le Online learning\n",
        "\n",
        "\n",
        "##      ----------          ----------            ----------\n",
        "##     |  Model 1 |        |  model 2 |   --->   |  Model X |\n",
        "##      ----------          ----------            ----------\n",
        "##          ^                    ^                    ^ \n",
        "##          |                    |                    |\n",
        "##\n",
        "##      une dataset 1       une dataset 2       une dataset X\n",
        "## \n",
        "## c'est pas vraiment la bonne demonstration ça mais ça fonctionne Qmm \n",
        "\n",
        "\n",
        "## dans ce cas là on augmente le nombre de samples à 1 million pour voir si ça fonctione avec un dataset si grand\n",
        "\n",
        "n_rows = 100000 * 11\n",
        "df = pd.read_csv(\"train.csv\", nrows=n_rows)\n",
        "\n",
        "\n",
        "\n",
        "df.columns = features\n",
        "\n",
        "X = df.drop(['state', 'id', 'C22', 'C23', 'C24'], axis=1).values\n",
        "Y = df['state'].values\n",
        "\n",
        "\n",
        "\n",
        "n_train = 100000 * 10\n",
        "X_train = X[:n_train]\n",
        "Y_train = Y[:n_train]\n",
        "X_test = X[n_train:]\n",
        "Y_test = Y[n_train:]\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "enc.fit(X_train)\n",
        "\n",
        "\n",
        "# le nombre d'iterations est mis à 1 si on utilise  partial_fit.\n",
        "sgd_lr_online = SGDClassifier(loss='log', penalty=None, fit_intercept=True, max_iter=1, learning_rate='constant',eta0=0.01)\n",
        "\n",
        "import timeit\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "\n",
        "# On utilise les premiers 1,000,000 samples pour le training, et le 100,000 qui rest pour le testing\n",
        "for i in range(10):\n",
        "    x_train = X_train[i*100000:(i+1)*100000]\n",
        "    y_train = Y_train[i*100000:(i+1)*100000]\n",
        "    x_train_enc = enc.transform(x_train)\n",
        "    sgd_lr_online.partial_fit(x_train_enc.toarray(), y_train, classes=[0, 1])\n",
        "\n",
        "print(f\"--- {(timeit.default_timer() - start_time)}.3fs secondes ---\")\n",
        "\n",
        "x_test_enc = enc.transform(X_test)\n",
        "\n",
        "pred = sgd_lr_online.predict_proba(x_test_enc.toarray())[:, 1]\n",
        "print(f'samples d entrainement: {n_train * 10}, AUC sur le Dataset TEST: {roc_auc_score(Y_test, pred):.3f}')\n",
        "\n",
        "## ============================= ====================== ================= ============== ##\n",
        "# la classification en utilisant la regression logistique \n",
        "\n",
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "n_samples = len(digits.images)\n",
        "X = digits.images.reshape((n_samples, -1))\n",
        "Y = digits.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'penalty': ['l2', None],\n",
        "              'alpha': [1e-07, 1e-06, 1e-05, 1e-04],\n",
        "              'eta0': [0.01, 0.1, 1, 10]}\n",
        "\n",
        "sgd_lr = SGDClassifier(loss='log', learning_rate='constant', eta0=0.01, fit_intercept=True, max_iter=100)\n",
        "\n",
        "grid_search = GridSearchCV(sgd_lr, parameters, n_jobs=-1, cv=5)\n",
        "\n",
        "grid_search.fit(X_train, Y_train)\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "sgd_lr_best = grid_search.best_estimator_\n",
        "accuracy = sgd_lr_best.score(X_test, Y_test)\n",
        "print(f'precision sur le dataset TEST: {accuracy*100:.1f}%')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 91.41519984199999.3fs secondes ---\n",
            "samples d entrainement: 10000000, AUC sur le Dataset TEST: 0.762\n",
            "{'alpha': 1e-05, 'eta0': 10, 'penalty': None}\n",
            "precision sur le dataset TEST: 96.1%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}